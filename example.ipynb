{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "082870c4",
   "metadata": {},
   "source": [
    "# Demo: Daft + Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a4309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ad7646-e194-42cf-99f9-8a29a99dd4a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACE_API_TOKEN\"] = \"*** YOUR TOKEN HERE ***\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb1d910",
   "metadata": {},
   "source": [
    "## Attach our catalog to the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec38462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from daft.io import IOConfig, HTTPConfig\n",
    "\n",
    "io_config = IOConfig(http=HTTPConfig(bearer_token=os.getenv(\"HUGGINGFACE_API_TOKEN\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09602fda-31fa-483d-8548-ef9d9101afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = daft.read_parquet(\"hf://datasets/laion/laion400m/**/*.parquet\", io_config=io_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e4163-ee58-478e-9ced-5d02df8b098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ca4523",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.limit(25)\n",
    "df = df.with_column(\n",
    "    \"image\",\n",
    "    df[\"url\"].url.download(on_error=\"null\").image.decode(on_error=\"null\").image.to_mode(\"RGB\"),\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1911619-08e5-4e13-ae82-7524782dcdfc",
   "metadata": {},
   "source": [
    "## Let's run an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6c209a-e528-4b86-86f1-59153b00cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are an AI assistant which powers an API for scoring the quality of image captions. Your capabilities\n",
    "includes determining the general overall quality of a caption,\n",
    "generating a cleaned-up version of the caption as well as being able to provide a reason for these scores.\n",
    "\n",
    "Caption quality is a scored float between 0 and 1, determined by:\n",
    "1. How well does a caption describe the overall scene of the image?\n",
    "2. Are semantically important focal points in the image captured by the caption?\n",
    "3. Does the caption make grammatical and logical sense?\n",
    "4. Is the caption not just describing what is in the image, but capturing important context around the significance of these items?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63546c26-ee8f-4e7c-8f72-a2b5252502fa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "import instructor\n",
    "\n",
    "max_concurrent_requests = 16\n",
    "\n",
    "import pydantic\n",
    "\n",
    "class CaptionScore(pydantic.BaseModel):\n",
    "    caption_quality: float\n",
    "    clean_caption: str\n",
    "    reason: str\n",
    "\n",
    "def resize_image(image, max_dimension = 1024) -> Image.Image:\n",
    "    width, height = image.size\n",
    "    if width > max_dimension or height > max_dimension:\n",
    "        if width > height:\n",
    "            new_width = max_dimension\n",
    "            new_height = int(height * (max_dimension / width))\n",
    "        else:\n",
    "            new_height = max_dimension\n",
    "            new_width = int(width * (max_dimension / height))\n",
    "        image = image.resize((new_width, new_height))\n",
    "    return image\n",
    "\n",
    "def convert_to_png(image):\n",
    "    with io.BytesIO() as output:\n",
    "        image.save(output, format=\"jpeg\")\n",
    "        return output.getvalue()\n",
    "\n",
    "def process_image(image) -> str:\n",
    "    resized_image = resize_image(image)\n",
    "    png_image = convert_to_png(resized_image)\n",
    "    return base64.b64encode(png_image).decode('utf-8')\n",
    "\n",
    "@daft.udf(return_dtype={\n",
    "    \"caption_similarity\": float,\n",
    "    \"caption_quality\": float,\n",
    "    \"clean_caption\": str,\n",
    "    \"reason\": str,\n",
    "})\n",
    "def score_caption(captions, images):\n",
    "    openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    client = instructor.from_openai(openai_client)\n",
    "    async def score_single_caption(caption, image_arr):\n",
    "        image = Image.fromarray(image_arr)\n",
    "        base64_encoded_image = process_image(image)\n",
    "        result = await client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            response_model=CaptionScore,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": caption},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_encoded_image}\"}},\n",
    "                ]},\n",
    "            ],\n",
    "        )\n",
    "        return result.model_dump()\n",
    "\n",
    "    async def analyze_with_semaphore(semaphore, *args):\n",
    "        async with semaphore:\n",
    "            return await score_single_caption(*args)\n",
    "\n",
    "    async def run_tasks():\n",
    "        semaphore = asyncio.Semaphore(max_concurrent_requests)\n",
    "        tasks = [\n",
    "            analyze_with_semaphore(semaphore, caption, image)\n",
    "            for caption, image in zip(captions, images)\n",
    "        ]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "    results = asyncio.run(run_tasks())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f306f4-b43b-419b-a307-6071668f20d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(\"image IS NOT NULL\")\n",
    "df = df.with_column(\"results\", score_caption(df[\"caption\"], df[\"image\"]))\n",
    "df = df.with_columns({\n",
    "    c: df[\"results\"][c]\n",
    "    for c in [\"caption_similarity\", \"caption_quality\", \"clean_caption\", \"reason\"]\n",
    "})\n",
    "df = df.select(\n",
    "    \"key\",\n",
    "    \"url\",\n",
    "    \"image\",\n",
    "    \"caption\",\n",
    "    \"clean_caption\",\n",
    "    \"similarity\",\n",
    "    \"caption_quality\",\n",
    "    \"reason\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac34e8-ac7a-4b53-b65d-61d6e6493a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2c4dc-fbb8-4572-bfc4-ce8262ebf7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.exclude(\"image\").write_parquet(\"laion-400M-sample/cleaned-captions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba3825-0a9d-4ae6-b43b-1201fcaab571",
   "metadata": {},
   "source": [
    "## Load into Pytorch for training\n",
    "\n",
    "Now that your data is \"cleaned\" and saved to your datalake, you can use Daft to stream it into your model during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb421dbc-5412-460b-8f35-721022e14292",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = daft.read_parquet(\"laion-400M-sample/cleaned-captions\")\n",
    "cleaned = cleaned.with_column(\n",
    "    \"image\",\n",
    "    df[\"url\"].url.download().image.decode(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdab33e-0f2d-4ebf-9de8-470b49fe2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.sort(\"caption_quality\", desc=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91030dd1-57fe-4dc7-9111-983d1a8fc98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = cleaned.where(\"caption_quality > 0.5\").to_torch_iter_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79324f4d-a064-4286-ae2c-6c040098204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(ds)\n",
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e12e9-5cd6-47d0-9dc3-4b659567975f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
